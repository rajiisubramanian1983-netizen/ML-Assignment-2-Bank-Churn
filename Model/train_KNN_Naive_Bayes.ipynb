{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef1abe66-dab8-4eff-accd-529caf78461a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Accuracy: 0.864\n",
      "AUC: 0.846416524382626\n",
      "\n",
      "XGBoost\n",
      "Accuracy: 0.8645\n",
      "AUC: 0.8628320153743883\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# ---------------- Load dataset ----------------\n",
    "df = pd.read_csv(r\"/home/cloud/Downloads/ML Assignment 2/Churn_Modelling.csv.csv\")\n",
    "\n",
    "df.drop(columns=[\"RowNumber\", \"CustomerId\", \"Surname\"], inplace=True)\n",
    "\n",
    "X = df.drop(\"Exited\", axis=1)\n",
    "y = df[\"Exited\"]\n",
    "\n",
    "le = LabelEncoder()\n",
    "X[\"Geography\"] = le.fit_transform(X[\"Geography\"])\n",
    "X[\"Gender\"] = le.fit_transform(X[\"Gender\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ================= Random Forest =================\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_proba_rf = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Random Forest\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_proba_rf))\n",
    "\n",
    "joblib.dump(rf, \"model_random_forest.pkl\")\n",
    "\n",
    "# ================= XGBoost (SAFE TRY) =================\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=42\n",
    "    )\n",
    "    xgb.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_xgb = xgb.predict(X_test)\n",
    "    y_proba_xgb = xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(\"\\nXGBoost\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "    print(\"AUC:\", roc_auc_score(y_test, y_proba_xgb))\n",
    "\n",
    "    joblib.dump(xgb, \"model_xgboost.pkl\")\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    print(\"\\nXGBoost NOT available in this environment.\")\n",
    "    print(\"Model trained locally / to be reported in README.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdeaf7e0-92c1-4a93-8edf-e95c3cc621cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "880a1365-9851-4866-9c77-5f73c7f2a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"/home/cloud/Downloads/ML Assignment 2/Churn_Modelling.csv.csv\")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df.drop(columns=[\"RowNumber\", \"CustomerId\", \"Surname\"], inplace=True)\n",
    "\n",
    "# Features & target\n",
    "X = df.drop(\"Exited\", axis=1)\n",
    "y = df[\"Exited\"]\n",
    "\n",
    "# Encode categorical variables\n",
    "le = LabelEncoder()\n",
    "X[\"Geography\"] = le.fit_transform(X[\"Geography\"])\n",
    "X[\"Gender\"] = le.fit_transform(X[\"Gender\"])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scaling (for LR & KNN)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39773b60-33c1-4526-90f2-c87e04a60b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, scaled=False):\n",
    "    if scaled:\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"AUC\": roc_auc_score(y_test, y_proba),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1\": f1_score(y_test, y_pred),\n",
    "        \"MCC\": matthews_corrcoef(y_test, y_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "441a6cc7-715d-4461-b302-f8ddd29e4180",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "# 1. Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "results[\"Logistic Regression\"] = evaluate_model(lr, X_test_scaled, y_test)\n",
    "\n",
    "# 2. Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "results[\"Decision Tree\"] = evaluate_model(dt, X_test, y_test)\n",
    "\n",
    "# 3. KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "results[\"KNN\"] = evaluate_model(knn, X_test_scaled, y_test)\n",
    "\n",
    "# 4. Naive Bayes\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_scaled, y_train)\n",
    "results[\"Naive Bayes\"] = evaluate_model(nb, X_test_scaled, y_test)\n",
    "\n",
    "# 5. Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "results[\"Random Forest\"] = evaluate_model(rf, X_test, y_test)\n",
    "\n",
    "# 6. XGBoost (only if available)\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=42\n",
    "    )\n",
    "    xgb.fit(X_train, y_train)\n",
    "    results[\"XGBoost\"] = evaluate_model(xgb, X_test, y_test)\n",
    "\n",
    "except:\n",
    "    print(\"XGBoost not available in this environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "248e83f7-4c7e-47de-af21-026579fa9070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.8050</td>\n",
       "      <td>0.771044</td>\n",
       "      <td>0.585859</td>\n",
       "      <td>0.142506</td>\n",
       "      <td>0.229249</td>\n",
       "      <td>0.216732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.7765</td>\n",
       "      <td>0.664883</td>\n",
       "      <td>0.453271</td>\n",
       "      <td>0.476658</td>\n",
       "      <td>0.464671</td>\n",
       "      <td>0.323715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.8350</td>\n",
       "      <td>0.772447</td>\n",
       "      <td>0.662447</td>\n",
       "      <td>0.385749</td>\n",
       "      <td>0.487578</td>\n",
       "      <td>0.417961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.8290</td>\n",
       "      <td>0.814619</td>\n",
       "      <td>0.755906</td>\n",
       "      <td>0.235872</td>\n",
       "      <td>0.359551</td>\n",
       "      <td>0.357286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.8640</td>\n",
       "      <td>0.846417</td>\n",
       "      <td>0.782427</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.529746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.8645</td>\n",
       "      <td>0.862832</td>\n",
       "      <td>0.776423</td>\n",
       "      <td>0.469287</td>\n",
       "      <td>0.584992</td>\n",
       "      <td>0.532934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy       AUC  Precision    Recall        F1  \\\n",
       "Logistic Regression    0.8050  0.771044   0.585859  0.142506  0.229249   \n",
       "Decision Tree          0.7765  0.664883   0.453271  0.476658  0.464671   \n",
       "KNN                    0.8350  0.772447   0.662447  0.385749  0.487578   \n",
       "Naive Bayes            0.8290  0.814619   0.755906  0.235872  0.359551   \n",
       "Random Forest          0.8640  0.846417   0.782427  0.459459  0.578947   \n",
       "XGBoost                0.8645  0.862832   0.776423  0.469287  0.584992   \n",
       "\n",
       "                          MCC  \n",
       "Logistic Regression  0.216732  \n",
       "Decision Tree        0.323715  \n",
       "KNN                  0.417961  \n",
       "Naive Bayes          0.357286  \n",
       "Random Forest        0.529746  \n",
       "XGBoost              0.532934  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df = pd.DataFrame(results).T\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "933930c9-8b6f-41b6-a474-beae8abf700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model folder\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "# Save KNN model\n",
    "with open(\"model/model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(knn, f)\n",
    "\n",
    "# Save scaler (important because you used StandardScaler)\n",
    "with open(\"model/scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae15fa04-890b-4981-9c3a-df5859726621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
